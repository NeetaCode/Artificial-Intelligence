# -*- coding: utf-8 -*-
"""AI Mid Term_ Neeta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bPbSq5ZvbB_hPZPomM9l5iIHI7rAJ8Sw
"""

# Importing necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
from keras.optimizers import Adam
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/AI_ Heart Diseases Prediction/AI_Dataset.csv")

# Display the first few rows of the dataset
print("Initial Dataset:")
print(df.head())

# Remove records with null values
df.dropna(inplace=True)

# Reset the indices to avoid potential issues
df = df.reset_index(drop=True)

# Standard Scalar (Z-score normalization)
scaler = StandardScaler()
df[df.columns[:-1]] = scaler.fit_transform(df[df.columns[:-1]])

# Outlier Detection and Removal (using z-score method)
z_scores = (df - df.mean()) / df.std()
outliers = (z_scores > 3) | (z_scores < -3)
df_cleaned = df[~outliers.any(axis=1)]

# One-Hot Encoding for categorical variables
categorical_cols = ['chest pain type', 'resting ecg', 'ST slope']
# categorical_cols = ['age', 'sex', 'chest pain type', 'resting bp s', 'cholesterol', 'fasting blood sugar', 'resting ecg', 'max heart rate', 'exercise angina', 'oldpeak', 'ST slope']
df_encoded = pd.get_dummies(df_cleaned, columns=categorical_cols)

# Display the cleaned dataset
print("Cleaned Dataset:")
print(df_cleaned.head())

# Extracting features and target variable
X = df_encoded.drop('target', axis=1)  # Features
y = df_encoded['target']  # Target variable

# Splitting the dataset into the training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize Logistic Regression with a higher max_iter value
logreg_model = LogisticRegression(max_iter=50000)

# Training the model
logreg_model.fit(X_train, y_train)

# Making predictions
y_pred = logreg_model.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Printing evaluation metrics
print("Logistic Regression Model Evaluation:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Apply PCA to reduce the dimensionality to two dimensions
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Train logistic regression model on PCA-transformed data
logreg_model.fit(X_train_pca, y_train)

# Plotting training data
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='coolwarm', edgecolors='k')
plt.xlabel('All the features')
plt.ylabel('Yes or No')

# Plotting decision boundary
x1_min, x1_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1
x2_min, x2_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1
xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.1), np.arange(x2_min, x2_max, 0.1))
Z = logreg_model.predict(np.c_[xx1.ravel(), xx2.ravel()])
Z = Z.reshape(xx1.shape)
plt.contourf(xx1, xx2, Z, alpha=0.4, cmap='coolwarm')
plt.xlim(xx1.min(), xx1.max())
plt.ylim(xx2.min(), xx2.max())
plt.title('Decision Boundary (PCA)')
plt.show()

#CODE WITH CNN
# Importing necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
from keras.optimizers import Adam

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/AI_ Heart Diseases Prediction/AI_Dataset.csv")

# Display the first few rows of the dataset
print("Initial Dataset:")
print(df.head())

# Remove records with null values
df.dropna(inplace=True)

# Reset the indices to avoid potential issues
df = df.reset_index(drop=True)

# Standard Scalar (Z-score normalization)
scaler = StandardScaler()
df[df.columns[:-1]] = scaler.fit_transform(df[df.columns[:-1]])

# Outlier Detection and Removal (using z-score method)
z_scores = (df - df.mean()) / df.std()
outliers = (z_scores > 3) | (z_scores < -3)
df_cleaned = df[~outliers.any(axis=1)]

# One-Hot Encoding for categorical variables
categorical_cols = ['chest pain type', 'resting ecg', 'ST slope']
df_encoded = pd.get_dummies(df_cleaned, columns=categorical_cols)

# Display the cleaned dataset
print("Cleaned Dataset:")
print(df_cleaned.head())

# Extracting features and target variable
X = df_encoded.drop('target', axis=1)  # Features
y = df_encoded['target']  # Target variable

# Splitting the dataset into the training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# CNN Model
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dense(2, activation='sigmoid'))  # Output layer with sigmoid for classification

# Compile the model with a low learning rate
optimizer = Adam(lr=0.0000001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Reshape data for CNN
X_train_cnn = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_cnn = np.array(X_test).reshape(X_test.shape[0], X_test.shape[1], 1)

# Model information
print("CNN Model Summary:")
print(model.summary())

# Train the CNN model
history = model.fit(X_train_cnn, y_train, epochs=50, batch_size=8, validation_data=(X_test_cnn, y_test), verbose=1)

# Evaluate the CNN model
y_pred_cnn = np.argmax(model.predict(X_test_cnn), axis=-1)
accuracy_cnn = accuracy_score(y_test, y_pred_cnn)
precision_cnn = precision_score(y_test, y_pred_cnn)
recall_cnn = recall_score(y_test, y_pred_cnn)
f1_cnn = f1_score(y_test, y_pred_cnn)

print("CNN Evaluation Metrics:")
print(f"Accuracy: {accuracy_cnn:.3f}")
print(f"Precision: {precision_cnn:.3f}")
print(f"Recall: {recall_cnn:.3f}")
print(f"F1-score: {f1_cnn:.3f}")

# Plotting training and testing accuracy for CNN
plt.plot(history.history['accuracy'], label='Training Accuracy (CNN)')
plt.plot(history.history['val_accuracy'], label='Testing Accuracy (CNN)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy (CNN)')
plt.legend()
plt.show()

# Plotting training and testing loss for CNN
plt.plot(history.history['loss'], label='Training Loss (CNN)')
plt.plot(history.history['val_loss'], label='Testing Loss (CNN)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Testing Loss (CNN)')
plt.legend()
plt.show()